import os
import streamlit as st
import openai
from googletrans import Translator

# --- 1. SEGURIDAD DEVOPS ---
api_key = os.getenv("OPENAI_API_KEY")

# --- 2. CONFIGURACI√ìN DE INTERFAZ ---
st.set_page_config(page_title="AI Trans Pro", page_icon="üåê")

with st.sidebar:
    st.image("logo_beta.png", width=150) if os.path.exists("logo_beta.png") else st.title("üåê AI Trans")
    st.info("üöÄ **Versi√≥n Beta v0.5.5**")
    
    # Si la clave es la de ejemplo o no est√°, pedimos la real
    if api_key and "sk-tu-clave" not in api_key:
        st.success("‚úÖ IA Conectada")
        openai.api_key = api_key
    else:
        st.warning("‚ö†Ô∏è Falta Key de OpenAI")
        api_key = st.text_input("OpenAI API Key:", type="password")
        openai.api_key = api_key

    st.markdown("---")
    motor = st.selectbox("Motor:", ["Google (Gratis)", "OpenAI (GPT-4)"])

# --- 3. CUERPO PRINCIPAL ---
st.title("üåê Traductor Pro Multi-Modo")
tab1, tab2, tab3 = st.tabs(["‚å®Ô∏è Texto", "üé§ Voz", "üì∏ Imagen"])

# Usamos session_state para que el texto persista entre pesta√±as
if "texto_capturado" not in st.session_state:
    st.session_state.texto_capturado = ""

with tab1:
    texto_usuario = st.text_area("Escribe o graba algo:", value=st.session_state.texto_capturado, height=150)

with tab2:
    st.write("### üéôÔ∏è Grabadora de Voz")
    # Este es el componente que ya tienes funcionando en tu captura
    audio_file = st.audio_input("Haz clic para hablar")
    
    if audio_file:
        if st.button("Transcribir mi voz ü§ñ"):
            if not api_key or "sk-" not in api_key:
                st.error("Necesitas una API Key real para transcribir.")
            else:
                try:
                    with st.spinner("Whisper est√° procesando tu audio..."):
                        # Guardar el audio temporalmente
                        with open("temp_audio.wav", "wb") as f:
                            f.write(audio_file.read())
                        
                        # Llamada a la API de Whisper
                        with open("temp_audio.wav", "rb") as audio:
                            transcripcion = openai.audio.transcriptions.create(
                                model="whisper-1", 
                                file=audio
                            )
                        
                        st.session_state.texto_capturado = transcripcion.text
                        st.success(f"Texto detectado: {transcripcion.text}")
                        st.rerun() # Refresca para que el texto aparezca en la Tab 1
                except Exception as e:
                    st.error(f"Error de procesamiento: {e}")

with tab3:
    st.info("üì∏ M√≥dulo de visi√≥n: Pr√≥ximamente.")

# --- 4. TRADUCCI√ìN ---
idioma_dest = st.selectbox("Idioma destino:", ["Spanish", "English", "French", "German"])

if st.button("TRADUCIR AHORA ‚ú®"):
    texto_a_procesar = st.session_state.texto_capturado if not texto_usuario else texto_usuario
    if not texto_a_procesar:
        st.warning("Escribe o graba algo primero.")
    else:
        with st.spinner('Traduciendo...'):
            try:
                if motor == "Google (Gratis)":
                    res = Translator().translate(texto_a_procesar, dest=idioma_dest[:2].lower()).text
                else:
                    response = openai.chat.completions.create(
                        model="gpt-4",
                        messages=[{"role": "user", "content": f"Translate to {idioma_dest}: {texto_a_procesar}"}]
                    )
                    res = response.choices[0].message.content
                st.success(f"### Resultado:\n{res}")
            except Exception as e:
                st.error(f"Hubo un problema: {e}")
